***COMMON OR ADVANTAGES OF NOSQL/HBASE
HUGE DATA,FAST RANDOM ACCESS,STRUCTURED DATA,VARIABLE SCHEMA,NEED OF COMPRESSION ,NEED OF DISTRIBUUTION.

***CHARACTRISTICS OF PRABABLE SOLUTION
DISTRIBUTED DATABASE,SORTED DATA,SPARE DATA STORE,AUTOMATIC SHADING.

*** DIFFRENCE BETWEEN 
  HBASE                          		    RDBMS
1.COLUMN ORIENTED             			ROW ORIENTED
2.FLEXIBLE SCHEMA ,ADD COLUMN ON FLY 		FIXED SCHEMA
3.GOOD WITH SPARE TABLE(HANDLE OVERHEAD 
NULL COLUMN)					NOT OPTIMIZED FOR SPARE TABLE
4.JOINS USING MR-NOT OPTIMIZED			OPTIMIZEDFOR JOINS
5.TIGHT INTEGRATION WITH MAP-REDUCE		NOT REALLY..
6.HORIZONTAL SCALABILITY			HARD TO SHARD AND SCALE 
7.GOOD FOR SEMI= STRUCTURED AND STRUCTURED DATA	GOOD FOR STRUCTURED DATA

***USE OF HBASE(USECASE)
1.UNSTRUCTURED DATA
2.HIGH VOLUME DATA TO BE STORED
3.COLUMN ORIENTED DATA
4.VERSIONED DATA
5.HIGH SCALABILITY
6.GENERATING DATA FROM MAP-REDUCE WORKFLOW

***WHEN NOT TO USE HBASE
1.WHEN YOU HAVE ONLY A FEW THOUSAND/MILLION ROWS.
2.LACKS RBMS COMMAND.
3.WHEN YOU HAVE HW LESS THAN 5 NODES WHEN REPLICTION FACTOR IS 3.

***IMPLEMENTATION USING HBASE
TWITTER,FACEBOOK ,YAHOO,TREND MICRO

***RUN HBASE(BASIC COMMAND)
1.CREATE TABLLE
>CREATE 'TABLE_NAME','COLUMN_FAMILY'(COLUMN NAME FOR EG SNAME,CNUM,CNAME AGE)

2.STORE DATA(INSERT DATA)
> PUT'TABLE-NAME','ROW_KEY','COLUMN_FAMILY:COLUMN_NAME','VALUE'

3.GET THE VALUE FROM HBASE
>1.SCAN(SELECT *)---SHOW WHOLE DATA
 SCAN'TABLE_NAME'
 2.GET 'TABLE_NAME,'ROW_KEY'

4.UPDATE/MODIFY
>PUT'TABLE_NAME','ROW_KEY','COLUMN_FAMILY:COLUMN_NAME','VALUE_MODIFIED'

5.DELETE DATA
> DELETE 'TABLE_NAME','ROW_KEY','COLUMN_FAMILY:COLUMN_NAME'

6.DROP/ALTER TABLE
DROP-
1.FIRST DISABLE THE TABLE
  DISABLE 'TABLE_NAME'
  DROP 'TABLE_NAME'


***HBASE ARCH
1.HBASE USES HDFS AS ITS RELIABLE STORAGE LAYER
 >HANLDLES CHECKSUMS,REPLICATION,FAILOVER
2.NATIVE JAVA API,GATEWAY FRO REST,THRIFT,AVRO
3.MASTER MANAGES CLUSTER
4.REGIONsERVER MANAGE DATA
5.ZOOKEEPER IS UDED THE 'NEURAL NETWORK'
  >CRUCIAL FOR HBASE
  >BOOTSTRAPS AND COORDINATES CLUSTER 


***AUTO SHARDING AND DISTRIBUTION
1.UNIT OF SCALABILITY IN HBASE IS THE REGION
2.SORTED,CONTIGUOUS RANGE OF ROWS
3.SPREAD'RANDOMLY' ACROSS REGIONSERVER
4.MOVED AROUND FOR LOAD BALANCING AND FAILOVER
5.SPLIT AUTOMATICALLY OR MANUALLY TO SCALE WITH GROWING DATA
6.CAPACITY IS SOLELY A FACTOR OF CLUSTER NODES VS REGIONS PER NODE. 

***STORAGE SEPRATION
COLUMN FAMILIES ALLOW FOR SEPRATION OF DATA 
      >USED BY COLUMNAR DATABASES FOR FAST ANALTICAL QUERIES BUT 		  ON COLUMN LEVEL ONLY.
	 >ALLOWS DIFFERENT OR NO COMPRESSION DEPENDING ON THE         	  CONTENT TYPE
   2.SEGREGRATE INFORMATION BASSED ON ACESS PATTERN
   3.DATA IS STORED IN ONE OE MORE STORAGE FILE,CALLED HFILES.

***BLOOM FILTER
 1.BLOOM FILTER ARE GENERATED WHEN HFILE IS PERSISTED
    -STORED AT THE END OF EACH HFILE
    -LOADED INTO MEMORY
 2.ALLOWS CHECK ON ROW OR ROW +COLUMN LEVEL
 3.CAN FILTER ENTIRE STORE FILES FROM READS
   USEFUL WHEN DATA IS GROUPED
 4.ALSO USEFUL WHEN MANY MISSES ARE EXPECTED DURING READS
   (NON EXISTING KEYS)

*** FOLD,STORE AND SHIFT
   1.LOGICAL LAYOUT DOES NOT MATCH PHYSICAL ON 
   2.ALL VALUES ARE STORED WITH THE FULL COORDINATED INCLUDING:
     ROW KEY,COLUMN-FAMILIY,COLUMN QUALIFIER AND TIMESTAMP.
   3.FOLD COLUMNS INTO 'ROW PER COLUMN'
   4.NULLS ARE COST FREE AS NOTHING IS STORED
   5.VERSION ARE MULTIPLE 'ROWS' IN FOLDED TABLE

***KEY CARDINALITY
1.THE BEST PERFORMANCE IS GAINED FROM USING ROW KEYS.
2.TIME RANGE BOUND READS CAN SKIP SOTRE FILES 
   -SO CAN BLOOM FILTERS
3.SELECTING COLUMN FAMILIES REDUCES THE AMT OF DATA TO BE SCANNED.
4.PURE VALUE BASSED FILTERING IS A FULL TABLE SCAN
   FILTERS OFTEN ARE TOO,BUT REDUCE NETWORK TRAFIC.

***TALL-NARROW VS FLAT-WIDE TABLES
1.ROWS DO NOT SPLIT
   -MIGHT END UP WITH ONE ROW PER REGION
2.SAME STORAGE FOOTPRINT
3.PUT MORE DETAILS INTO THE ROW KEY
   -SOMETIMES DUMMY COLUMN ONLY
   -MAKE USE OF PARTIAL KEY SCANS.
4.TALL WITH SCANS,WIDE WITH GETS
   -ATOMICITY ONLY ON ROW LEVEL
5.EG: LARGE GRAPH,STORES AS ADJANCENCY MATRIX.
   MAIL INBOX....

***SEQUENTIAL KEYS
<TIMESTAMP><MORE KEY>:{CF:{CQ:{TS:VAL}}}
1.HOTSPOTTING ON REGIONS:BAD!
2.INSTEAD DO ONE OF THE FOLLOWINNG
  >SALTING:
    -PREFIX <TIMESTAMP> WITH DISRIBUTED VALUE
    -BINNING OR BUCKETING ROWS ACROSS REGIONS
  >KEY FIELD SWAP/PROMOTION
    -MOVE<MORE KEY>BEFORE THE TIMESTAMP (SEE OPEN TSDB LATER)
  >RANDOMIZATION
    -MOVE<TIMESTAMP> OUT OF KEY

*** KEY DESIGN
 1.BASSED ON ACCESS PATTERN,EITHER USE SEQUENTIAL OR RANDOM KEYS
 2.OFTEN A COMBINATION OF BOTH IS NEEDED
  - OVERCOME ARCH LIMITATION
 3.NEITHER IS NECESSARILY BAD
  - USE BULK IMPORT FOR SEQUENTIAL KEY AND READS
  -RANDOM KEYS ARE GOOD FOR RANDOM ACCESS PATTERNS.
 
1.REVERSED DOMAINS- HELPS KEEPING PAGES PER SITE CLOSE,AS HBASE 				    EFFICIENTLY SCANS BLOCKS OF SORTED KEYS.
2.DOMAIN ROW KEY:- MD5(REVERSED DOMAIN)+REVERSED DOMAIN.
			   >LEADING MD5 HASH SPREADS KEYS RANDOMLY ACROSS 			    ALL REGION FOR LOAD BALANCING .
			   >ONLY HASHING THE DOMAIN GROUPS PER SITE

3.URL ROW KEY :- MD5(REVERSED DOMAIN)+REVERSED DOMAIN+URL ID
               >UNIQUE ID PER URL ALREADY AVAILABLE ,MAKE USE OF 				IT.

***HBASE COUNTERS
1.STRE COUNTERS PER DOMAIN AND PER URL
2.EACH ROW IS SPECIFIC DOMAIN OR URL
3.THE COLUMN ARE THE COUNTERS FOR SPECIFIC METRICS
4.COLUMN FAMILIES ARE USED TO GROUP COUNTERS BY TIME RANGE


***SUMMARY 
--DESIGN FOR USE-CASE
  READ,WRITE,OR BOTH?
--AVOID HOTSPOTTING
--CONSIDER USING IDS INSTEAD OF FULL TEXT
--LEVERAGE COLUMN FAMILY TO HFile RELATION 
--SHIFT DETAILS TO APPROPRIATE POSITION
  >COMPOSITE KEYS
  >COLUMN QUALIFIERS.
--SCHEMA DESIGN IS COMBINATION OF
  > DESIGNING THE KEYS 
  >SEGREGATE FDATA INTO COLOUMN FAMILIES
  >CHOOSE COMPRESSION AND BLOCK SIZES
--SIMILAR TECH ARE NEEDES TO SCALE MOST SYS.
 > ADD INDEXES ,PARTITION DATA,CONSISTENT HASHING
--DENORMALIZATION,DUPLICATION,AND INTELLIGENT KEY(DDI).



14/12/2016----Lecture

THREE DIFF TYPES OF DATA
1.STRUCTURED--RDBMS DATA
2.SEMI-STRUCTURED--MOVIE ,VIDEO
3.UNSTRUCTURED--XML SHEET

BIGDATA->BIGDATA IS AN ALL ENCOMPASSING TERM FOR ANY COLLECTION OF DATA SETS SO LARGE AND 
COMPLEX THAT IT BECOMES DIFFICULT TO PROCESS THEM USING TRADITIONAL DATA PROCESSING APPLICATIONS.

CHARACTRISTICS OF BIG DATA->
1.VOLUME:SCALE OF DATA
2.VELOCITY:ANALYSIS OF STREAMING DATA
3.VARIETY:DIFFRENT FORMS OF DATA
4.VERACITY:UNCERTAINITY OF DATA.
5.VALUE:

BIGDATA ANALYTICS->BIGDATA ANALYTICS IS THE PROCESS OF EXAMING LARGE DATA SETS CONTAINING 
A VARIETY OF DATA TYPES==I.E BIGDATA--TO UNCOVER HIDDEN PATTERNS,UNKNOWN CERRELATIONS,
MARKET TRENDS,CUSTOMER PREFERENCE AND OTHER USEFULL..


CHALLENGES OF BIGDATA->ANALYSIS,CAPTURE,SEARCH,SHARING,STORAGE,TRANSFER,
VISUALIZATIONAND PRIVACY VIOLATIONS. 


HADOOP HAS 2 COMPONENT->
1.HDFS-FOR STORAGE
2.MAPREDUCE-PROCESSING FRAMEWORK

CLUSTER-IT IS NOTHINH BUT NETWORK OF MACHINES EACH OF THEM HAVING THEIR OWN PRIMARY AND SECONDARY STORAGE CAPACITY.
 IS IS BASICALLY GROUP OF NODE OR LARGE SET OF NETWORK...


HOW TO MAP THE BLOCK SIZE->
BLOCK SIZE=128 MB
1024/128 MB=8BLOCKS\CLUSTER=30 MACHINES*10TB=300 TB DATANODES


MASTER-SLAVE ARCH->
IT CONTAIN TWO DIFFRENT NODES
1.MASTER NODE- NAMENODE(CONTROL ENTIRE STRUCTURE)KEEP THE TRACK OF BLOCK ID,LOCATION,AL INFORMATION OF METADATA(DOMAIN)
2.SLAVES NODES-DATANODES(IT IS ACTUAL DATA IN THE FORM OF BLOCK) 


***CHUNKS IS NOTHING BUT BLOCK ...

EVERY BLOCK CONTAINING 3 THREE COPY(REPLICA)OF BLOCKS.


LAYERS OF BIGDATA
1.SOURCE LAYER--> TRANSACTIONDATA,MASTER DATA,SOCIAL MEDIA DATA.
2.DATA INGESTION LAYER-->SQOOP
3.STORAGE AND PROCESSING-->STROM,SPARK
4.SRVICE LAYER-->REAL-TIME, BATCH VIEW
5.VISUALIZATION LAYER-REAL-TIME QUERY,ANALYTICS REQUEST.




HADOOP COMPONENT OVERVIEW
1.HUE-CLOUDERA
2.ZOOKEEPER-APACHE
3.SQOOP-CLOUDERA
4.HDFS-APACHE
5.MAP REDUCE-GOOGLE
6.HADOOP COMMON
7.HIVE
8.PIG
9.OOZIE

***HADOOP ECOSYSTEM-->REFER WILEY BOOK

***HADOOP IS A OPEN SOURCE SOFTWARE FRAMEWORK USED FOR STORING AND PROCESSING THE BIGDATA IN A DISTRIBUTED WAY ON A CLUSTER COMMODITY HW.



